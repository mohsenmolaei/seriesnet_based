{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohsenmolaei/seriesnet_based/blob/main/seriesNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFy2ZqO3BizP",
        "outputId": "213a4182-6db4-4aac-8381-7c949de3e66a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/.shortcut-targets-by-id/15DygMAXOJIypb4JGFSiPmdC6GqJjdD9a/tripple stage seriesnet\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd \"/content/drive/MyDrive/Colab Notebooks/tripple stage seriesnet\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNtWkhcB-lhy",
        "outputId": "f68f61e1-cf82-43be-89cf-0cc22cc4a6a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting lingam\n",
            "  Downloading lingam-1.7.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lingam) (1.10.1)\n",
            "Collecting factor-analyzer\n",
            "  Downloading factor_analyzer-0.4.1.tar.gz (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pygam\n",
            "  Downloading pygam-0.9.0-py3-none-any.whl (522 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m522.2/522.2 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from lingam) (0.20.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from lingam) (1.2.2)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from lingam) (0.13.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from lingam) (3.1)\n",
            "Requirement already satisfied: numpy<1.23.5 in /usr/local/lib/python3.10/dist-packages (from lingam) (1.22.4)\n",
            "Collecting igraph\n",
            "  Downloading igraph-0.10.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from lingam) (1.5.3)\n",
            "Collecting pre-commit\n",
            "  Downloading pre_commit-3.3.1-py2.py3-none-any.whl (202 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.5/202.5 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting texttable>=1.6.2\n",
            "  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->lingam) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->lingam) (2022.7.1)\n",
            "Requirement already satisfied: progressbar2<5.0.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pygam->lingam) (4.2.0)\n",
            "Collecting pygam\n",
            "  Downloading pygam-0.8.1-py3-none-any.whl (522 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m522.2/522.2 kB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading pygam-0.8.0-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pygam->lingam) (0.18.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->lingam) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->lingam) (3.1.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels->lingam) (23.1)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels->lingam) (0.5.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.2->statsmodels->lingam) (1.16.0)\n",
            "Collecting cfgv>=2.0.0\n",
            "  Downloading cfgv-3.3.1-py2.py3-none-any.whl (7.3 kB)\n",
            "Collecting identify>=1.0.0\n",
            "  Downloading identify-2.5.24-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.8/98.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->factor-analyzer->lingam) (6.0)\n",
            "Collecting nodeenv>=0.11.1\n",
            "  Downloading nodeenv-1.7.0-py2.py3-none-any.whl (21 kB)\n",
            "Collecting virtualenv>=20.10.0\n",
            "  Downloading virtualenv-20.23.0-py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-utils>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from progressbar2<5.0.0,>=4.2.0->pygam->lingam) (3.5.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nodeenv>=0.11.1->pre-commit->factor-analyzer->lingam) (67.7.2)\n",
            "Collecting distlib<1,>=0.3.6\n",
            "  Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.5/468.5 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock<4,>=3.11 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->factor-analyzer->lingam) (3.12.0)\n",
            "Requirement already satisfied: platformdirs<4,>=3.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->factor-analyzer->lingam) (3.3.0)\n",
            "Building wheels for collected packages: factor-analyzer\n",
            "  Building wheel for factor-analyzer (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for factor-analyzer: filename=factor_analyzer-0.4.1-py2.py3-none-any.whl size=42034 sha256=21d63cadf07b10ea02b49dc6e926b3ab95bccb7a74c6a9e07b6a53323e5af579\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/94/da/41abe415f64706710726291086a814dd8b9e0dab1c491ef6ed\n",
            "Successfully built factor-analyzer\n",
            "Installing collected packages: texttable, distlib, virtualenv, nodeenv, igraph, identify, cfgv, pygam, pre-commit, factor-analyzer, lingam\n",
            "Successfully installed cfgv-3.3.1 distlib-0.3.6 factor-analyzer-0.4.1 identify-2.5.24 igraph-0.10.4 lingam-1.7.1 nodeenv-1.7.0 pre-commit-3.3.1 pygam-0.8.0 texttable-1.6.7 virtualenv-20.23.0\n",
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "import pdb\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder ,StandardScaler\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "# !pip install yfinance\n",
        "# import yfinance as yf\n",
        "!pip install lingam\n",
        "\n",
        "import TSASeriesNet\n",
        "import feature_selection\n",
        "if torch.cuda.is_available():\n",
        "    # dev = \"TPU:0\"\n",
        "    dev = \"cuda:0\" \n",
        "else: \n",
        "    dev = \"cpu\" \n",
        "device = torch.device(dev) \n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHyrsSKOp01U"
      },
      "outputs": [],
      "source": [
        "data= pd.read_csv('data.csv')\n",
        "data.set_index(\"time_stamp\",inplace=True,drop=True)\n",
        "# data\n",
        "# label_encoder = LabelEncoder()\n",
        "# n_bins = 50\n",
        "# data['difficulty_latest'] = data['difficulty_latest'].astype(float)\n",
        "# data['hash_rate_mean'] = data['hash_rate_mean'].astype(float)\n",
        "\n",
        "# data['difficulty_latest'] = label_encoder.fit_transform(pd.cut(data['difficulty_latest'] , n_bins, retbins=True)[0])\n",
        "# data['hash_rate_mean'] = label_encoder.fit_transform(pd.cut(data['hash_rate_mean'], n_bins, retbins=True)[0])\n",
        "\n",
        "# whatcol=[ 'price_drawdown_relative','difficulty_latest',\n",
        "#            'hash_rate_mean', 'active_more_1y_percent', 'utxo_created_value_median', \n",
        "#            'transfers_volume_median', 'utxo_spent_value_median']\n",
        "\n",
        "X, Y = data.drop([\"price_usd_close\"],axis =1), np.array(data.price_usd_close) #data[whatcol]\n",
        "# X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zJIM3lXSqgB"
      },
      "outputs": [],
      "source": [
        "# autoscaler = StandardScaler()\n",
        "# features = autoscaler.fit_transform(data)\n",
        "\n",
        "# label_encoder = LabelEncoder()\n",
        "# n_bins = 50\n",
        "# y_train = label_encoder.fit_transform(pd.cut(Y, n_bins, retbins=True)[0])\n",
        "\n",
        "# mi= pd.DataFrame()\n",
        "# mi = mutual_info_classif(X, y_train)#, discrete_features=discrete_vars)\n",
        "# mi = pd.Series(mi)\n",
        "\n",
        "# mi.index = col\n",
        "# mi.sort_values(ascending=False).plot.bar(figsize=(15, 10))\n",
        "# plt.ylabel('Mutual Information')\n",
        "# plt.title(f\"Mutual information between predictors and target.  bins: {n_bins} \")\n",
        "\n",
        "# plt.show()\n",
        "\n",
        "# # xindex = mi.sort_values(ascending=False)[0:8]\n",
        "# # X = data[xindex.index]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXQhvh75p4Qs"
      },
      "outputs": [],
      "source": [
        "data_top15= pd.read_csv('top15.csv')\n",
        "data_top15.set_index(\"time_stamp\",inplace=True,drop=True)\n",
        "\n",
        "# autoscaler = StandardScaler()\n",
        "# features = autoscaler.fit_transform(data_top15)\n",
        "\n",
        "# label_encoder = LabelEncoder()\n",
        "# n_bins = 50\n",
        "\n",
        "# mi15= pd.DataFrame()\n",
        "# mi15 = mutual_info_classif(data_top15, y_train)#, discrete_features=discrete_vars)\n",
        "# mi15 = pd.Series(mi15)\n",
        "\n",
        "# mi15.index = data_top15.columns\n",
        "# mi15.sort_values(ascending=False).plot.bar(figsize=(15, 10))\n",
        "# plt.ylabel('Mutual Information')\n",
        "# plt.title(f\"Mutual information between predictors and target.  bins: {n_bins} \")\n",
        "\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIij7foycRHa"
      },
      "outputs": [],
      "source": [
        "ccol = ['CL=F', 'CPI', 'DXYN', 'GC=F', 'SI=F', '^DJI', '^IXIC', '^RUT', '^TNX','EURUSD=X', 'GBPUSD=X', '^FTSE', 'N225']\n",
        "\n",
        "Condition= pd.read_csv('Condition.csv')\n",
        "Condition.set_index(\"time_stamp\",inplace=True,drop=True)\n",
        "# Condition\n",
        "\n",
        "# autoscaler = StandardScaler()\n",
        "# features = autoscaler.fit_transform(Condition)\n",
        "\n",
        "# label_encoder = LabelEncoder()\n",
        "# n_bins = 50\n",
        "# y_train = label_encoder.fit_transform(pd.cut(Y, n_bins, retbins=True)[0])\n",
        "\n",
        "# mi= pd.DataFrame()\n",
        "# mi = mutual_info_classif(Condition, y_train)#, discrete_features=discrete_vars)\n",
        "# mi = pd.Series(mi)\n",
        "\n",
        "# mi.index = Condition.columns\n",
        "# mi.sort_values(ascending=False).plot.bar(figsize=(15, 10))\n",
        "# plt.ylabel('Mutual Information')\n",
        "# plt.title(f\"Mutual information between predictors and target.  bins: {n_bins} \")\n",
        "# plt.show()\n",
        "\n",
        "# xindex = mi.sort_values(ascending=False)[0:8]\n",
        "# Condition = data[xindex.index]\n",
        "\n",
        "# Condition.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7XXU-uiPMBe"
      },
      "outputs": [],
      "source": [
        "col = list(data.columns)+ list(Condition.columns)+list(data_top15.columns)\n",
        "cond = pd.DataFrame(np.hstack((data , Condition)))\n",
        "cond = pd.DataFrame(np.hstack((cond,data_top15)))\n",
        "cond.columns =col\n",
        "\n",
        "import importlib\n",
        "importlib.reload(feature_selection)\n",
        "\n",
        "crypto = \"price_usd_close\"\n",
        "selectmx , matrix_correlation , matrix_observed , matrix_latent , matrix_granger , matrix_mi = feature_selection.select_matrix(cond, crypto)\n",
        "cond =cond.drop([\"price_usd_close\"],axis =1)\n",
        "# selectmx.to_csv(\"select_matrix.csv\")\n",
        "\n",
        "# matrix_correlation.to_csv(\"matrix_correlation.csv\")\n",
        "# matrix_observed.to_csv(\"matrix_observed.csv\")\n",
        "# matrix_latent.to_csv(\"matrix_latent.csv\")\n",
        "# matrix_granger.to_csv(\"matrix_granger.csv\")\n",
        "# matrix_mi.to_csv(\"matrix_mi.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiC709vaog2Z",
        "outputId": "8d009d93-3a90-4b2e-963d-9edaecf055ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2643, 1)\n",
            "(2643, 50)\n",
            "(2643, 1)\n",
            "(2606, 30, 1) (2606, 30, 50) (2606, 7, 1)\n",
            "529\n",
            "Training Shape: (2077, 30, 1) (2077, 7, 1)\n",
            "Testing Shape: (529, 30, 1) (529, 7, 1)\n",
            "torch.Size([2077, 30, 1])\n",
            "torch.Size([2077, 30, 50])\n",
            "Training Shape: torch.Size([2077, 30, 1]) torch.Size([2077, 30, 50]) torch.Size([2077, 7, 1])\n",
            "Testing Shape: torch.Size([529, 30, 1]) torch.Size([529, 30, 50]) torch.Size([529, 7, 1])\n"
          ]
        }
      ],
      "source": [
        "\"\"\"# scale features\"\"\" \n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "MMScaler = MinMaxScaler()\n",
        "SScaler = StandardScaler()\n",
        "condscaler = StandardScaler()\n",
        "XX = np.array(data.price_usd_close)\n",
        "\n",
        "X_trans = SScaler.fit_transform(XX.reshape(-1, 1))#Y.reshape(-1, 1))\n",
        "condtemp = cond\n",
        "c_trans = condscaler.fit_transform(condtemp)\n",
        "\n",
        "for i in range(0,len(selectmx.T)):\n",
        "  col2 = c_trans[:,i].copy()\n",
        "  if selectmx.T[\"price_usd_close\"][i] >= max(selectmx.T[\"price_usd_close\"]) :\n",
        "    print(col[i])\n",
        "    col2 *= selectmx.T[\"price_usd_close\"][i]/max(selectmx.T[\"price_usd_close\"])\n",
        "  else:\n",
        "    col2 *= 0\n",
        "  c_trans[:, i] = col2\n",
        "print(\"\\n\\n\")\n",
        "c_trans = pd.DataFrame(c_trans)\n",
        "c_trans = c_trans.drop(c_trans.columns[c_trans.apply(lambda col: col.all() == 0)], axis=1)\n",
        "\n",
        "y_trans = MMScaler.fit_transform(Y.reshape(-1, 1))\n",
        "print(X_trans.shape)\n",
        "print(c_trans.shape)\n",
        "print(y_trans.shape)\n",
        "\n",
        "\"\"\"# split a multivariate sequence past, future samples (X and y)\"\"\"\n",
        "def split_sequences(input_sequences, condition_seq, output_sequence, n_steps_in, n_steps_out):\n",
        "    X, C, y = list(),list(), list() \n",
        "    for i in range(len(input_sequences)):\n",
        "        end_ix = i + n_steps_in\n",
        "        out_end_ix = end_ix + n_steps_out \n",
        "        if out_end_ix >= len(input_sequences): break\n",
        "        seq_x, seq_c, seq_y = input_sequences[i:end_ix], condition_seq[i:end_ix], output_sequence[end_ix:out_end_ix]\n",
        "        X.append(seq_x), C.append(seq_c) ,y.append(seq_y)\n",
        "    return np.array(X), np.array(C), np.array(y)\n",
        "\n",
        "x_shape = 30\n",
        "y_shape = 7\n",
        "X_ss, C_ss, y_mm = split_sequences(X_trans, c_trans, y_trans, x_shape, y_shape)\n",
        "print(X_ss.shape, C_ss.shape, y_mm.shape)\n",
        "\n",
        "total_samples = len(X)\n",
        "train_test_cutoff = round(0.80 * total_samples)\n",
        "\n",
        "X_train = X_ss[:-(total_samples-train_test_cutoff)]\n",
        "C_train = C_ss[:-(total_samples-train_test_cutoff)]\n",
        "X_test = X_ss[-(total_samples-train_test_cutoff):]\n",
        "C_test = C_ss[-(total_samples-train_test_cutoff):]\n",
        "\n",
        "y_train = y_mm[:-(total_samples-train_test_cutoff)]\n",
        "y_test = y_mm[-(total_samples-train_test_cutoff):] \n",
        "\n",
        "print(total_samples - train_test_cutoff)\n",
        "print(\"Training Shape:\", X_train.shape, y_train.shape)\n",
        "print(\"Testing Shape:\", X_test.shape, y_test.shape)\n",
        "\n",
        "\"\"\"# convert to pytorch tensors\"\"\"\n",
        "X_train_tensors = Variable(torch.Tensor(X_train).to(device))\n",
        "X_test_tensors = Variable(torch.Tensor(X_test).to(device))\n",
        "\n",
        "C_train_tensors = Variable(torch.Tensor(C_train).to(device))\n",
        "C_test_tensors = Variable(torch.Tensor(C_test).to(device))\n",
        "\n",
        "y_train_tensors = Variable(torch.Tensor(y_train).to(device))\n",
        "y_test_tensors = Variable(torch.Tensor(y_test).to(device))\n",
        "\n",
        "print(X_train_tensors.shape)\n",
        "print(C_train_tensors.shape)\n",
        "\n",
        "\"\"\"# reshaping to rows, timestamps, features\"\"\"\n",
        "\n",
        "X_train_tensors_final = torch.reshape(X_train_tensors,   \n",
        "                                      (X_train_tensors.shape[0], x_shape, \n",
        "                                       X_train_tensors.shape[2]))\n",
        "X_test_tensors_final = torch.reshape(X_test_tensors,  \n",
        "                                     (X_test_tensors.shape[0], x_shape, \n",
        "                                      X_test_tensors.shape[2])) \n",
        "\n",
        "C_train_tensors_final = torch.reshape(C_train_tensors,   \n",
        "                                      (C_train_tensors.shape[0], x_shape, \n",
        "                                       C_train_tensors.shape[2]))\n",
        "C_test_tensors_final = torch.reshape(C_test_tensors,  \n",
        "                                     (C_test_tensors.shape[0], x_shape, \n",
        "                                      C_test_tensors.shape[2])) \n",
        "\n",
        "print(\"Training Shape:\", X_train_tensors_final.shape, C_train_tensors_final.shape, y_train_tensors.shape)\n",
        "print(\"Testing Shape:\", X_test_tensors_final.shape, C_test_tensors_final.shape, y_test_tensors.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggEJ_DwqBnm0"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "def NRMSELoss(yhat,y): #NRMSE\n",
        "  y = y.reshape(y.shape[0], y.shape[1])\n",
        "  return (torch.sqrt(torch.mean(torch.square(yhat-y))))/ (torch.max(y) - torch.min(y))\n",
        "\n",
        "# Root Mean Squared Percentage Error (RMSPE)\n",
        "def rmspe(y_pred,y_true):\n",
        "  y_true = y_true.reshape(y_true.shape[0], y_true.shape[1])\n",
        "  return torch.sqrt(torch.mean(torch.square((y_true - y_pred) / y_true)))\n",
        "\n",
        "def MAPELoss(yhat,y):\n",
        "  y = y.reshape(y.shape[0], y.shape[1])\n",
        "  return torch.mean(torch.abs(yhat-y)/torch.abs(y))\n",
        "\n",
        "def SMAPELoss(y_pred, y_true):\n",
        "  y_true = y_true.reshape(y_true.shape[0], y_true.shape[1])\n",
        "  loss = 2 * torch.mean(torch.abs(y_true - y_pred) / (torch.max(y_true) + torch.max(y_true)))\n",
        "  return loss\n",
        "\n",
        "def MSELoss(y_pred, y_true):\n",
        "  y_true = y_true.reshape(y_true.shape[0], y_true.shape[1])\n",
        "  loss = torch.mean((y_pred - y_true)**2)\n",
        "  return loss\n",
        "\n",
        "def RMSELoss(y_pred, y_true):\n",
        "  y_true = y_true.reshape(y_true.shape[0], y_true.shape[1])\n",
        "  loss = torch.sqrt(torch.mean((y_pred - y_true)**2))\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWEL0qCuHuYw"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorboard\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter('logs')\n",
        "\n",
        "\n",
        "def exp_lr_scheduler(optimizer, epoch, init_lr=0.01, lr_decay_epoch=200):\n",
        "    lr = init_lr * (0.9**(epoch // lr_decay_epoch))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    return optimizer\n",
        "\n",
        "def training_loop(n_epochs,learning_rate, lr_decay_epoch, network, optimiser, loss_fn, X_train, Condition_train, y_train,X_test, Condition_test, y_test):\n",
        "    loss_valid_show,loss_train_show = [],[]\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        network.cuda()\n",
        "    best_valid_loss = float('inf')\n",
        "    best_epoch = -1\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        if (epoch % (lr_decay_epoch) == 0) and (epoch != 0):\n",
        "            optimiser = exp_lr_scheduler(optimiser,epoch, init_lr= learning_rate , lr_decay_epoch= lr_decay_epoch)\n",
        "\n",
        "        network.train()\n",
        "        outputs = network.forward(X_train, Condition_train) # forward pass\n",
        "        optimiser.zero_grad() # calculate the gradient, manually setting to 0\n",
        "        # obtain the loss function\n",
        "        \n",
        "        # pdb.set_trace()\n",
        "        loss = loss_fn(outputs, y_train)\n",
        "        loss.backward() # calculates the loss of the loss function\n",
        "        # print(optimiser.lr)\n",
        "\n",
        "        optimiser.step() # improve from loss, i.e backprop\n",
        "        # test loss\n",
        "        network.eval()\n",
        "        test_preds = network(X_test, Condition_test)\n",
        "        test_loss = loss_fn(test_preds, y_test)\n",
        "\n",
        "        loss_valid_show.append(test_loss.item())\n",
        "        loss_train_show.append(loss.item())\n",
        "\n",
        "        # writer.add_scalar('Accuracy/train', acc, epoch)\n",
        "        writer.add_scalar('Loss/Validation', test_loss.item(), epoch)\n",
        "        writer.add_scalar('Loss/Train', loss.item(), epoch)\n",
        "\n",
        "        # Save model weights if validation loss improves\n",
        "        if test_loss < best_valid_loss:\n",
        "            best_valid_loss = test_loss\n",
        "            best_epoch = epoch\n",
        "            torch.save({'epoch': epoch, 'state_dict': network.state_dict()}, f'./weights/model_epoch_{epoch}.pth')\n",
        "\n",
        "\n",
        "        if (epoch) % (20) == 0:\n",
        "            print(\"Epoch: %4d, train loss: %1.5f, test loss: %1.5f\" % (epoch, loss.item(),test_loss.item()))\n",
        "    writer.close()\n",
        "    return loss_train_show , loss_valid_show , best_valid_loss , best_epoch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAuPumvpEzYG",
        "outputId": "51510722-955f-4b66-a5f8-827f23fdc334"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "124"
            ]
          },
          "metadata": {},
          "execution_count": 191
        }
      ],
      "source": [
        "\"\"\"# Training\"\"\"\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qV8DvLtJFSfr"
      },
      "outputs": [],
      "source": [
        "# torch.manual_seed(17)\n",
        "# torch.cuda.manual_seed(17)\n",
        "# np.random.seed(17)\n",
        "# torch.backends.cudnn.deterministic=True\n",
        "\n",
        "import importlib\n",
        "import Decoder_DARLM\n",
        "import Encoder_DARLM\n",
        "import ts_lstm\n",
        "import ts_gru\n",
        "import HSAM\n",
        "import CBAM\n",
        "\n",
        "importlib.reload(CBAM)\n",
        "importlib.reload(Decoder_DARLM)\n",
        "importlib.reload(TSASeriesNet)\n",
        "importlib.reload(Encoder_DARLM)\n",
        "importlib.reload(ts_lstm)\n",
        "importlib.reload(ts_gru)\n",
        "importlib.reload(HSAM)\n",
        "\n",
        "n_epochs = 221\n",
        "num_inputs = X_train_tensors_final.shape[1] \n",
        "dilation_c = 2\n",
        "kernel_size_EN = 3\n",
        "kernel_size_DE = 3\n",
        "hidden_size_lstm =20\n",
        "\n",
        "num_levels_en = 5\n",
        "num_levels_de = 5\n",
        "num_layers_lstm = 3\n",
        "num_layers_gru = 3\n",
        "features = X_train_tensors_final.shape[2]   \n",
        "features_c = C_train_tensors_final.shape[2] \n",
        "output_num = y_train_tensors.shape[1]\n",
        "\n",
        "lr_decay_epoch = 49\n",
        "learning_rate = 0.01\n",
        "weight_decay = 0.0001\n",
        "\n",
        "loss_fn = RMSELoss #SMAPELoss #MAPELoss #NRMSELoss #rmspe #RMSELoss\n",
        "# ANN model \n",
        "myModel = TSASeriesNet.ANNmodel(num_inputs, features_c, features, output_num, num_levels_en,num_levels_de, kernel_size_EN, kernel_size_DE, dilation_c, hidden_size_lstm, num_layers_lstm, num_layers_gru ).to(device)\n",
        "optimiser = torch.optim.Adam(myModel.parameters())#,weight_decay=weight_decay)#,eps=1e-08)#, lr=learning_rate)#, eps=1e-08)#\n",
        "\n",
        "loss_train_show ,loss_valid_show , best_valid_loss , best_epoch= training_loop(n_epochs=n_epochs,\n",
        "                                  learning_rate = learning_rate,\n",
        "                                  lr_decay_epoch = lr_decay_epoch,\n",
        "                                  network=myModel,\n",
        "                                  optimiser=optimiser,\n",
        "                                  loss_fn=loss_fn,\n",
        "                                  X_train=X_train_tensors_final,\n",
        "                                  Condition_train = C_train_tensors_final,\n",
        "                                  y_train=y_train_tensors,\n",
        "                                  X_test=X_test_tensors_final,\n",
        "                                  Condition_test  = C_test_tensors_final,\n",
        "                                  y_test=y_test_tensors)\n",
        "\n",
        "print(f\"best validation loss: {best_valid_loss} in epoch {best_epoch}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvWDvRKhnWpR"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOM8dKj_9_cq"
      },
      "outputs": [],
      "source": [
        "checkpoint = torch.load(f'./weights/model_epoch_{best_epoch}.pth')\n",
        "myModel.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "predict = myModel(X_test_tensors_final,C_test_tensors_final)\n",
        "predict = predict.detach().cpu().numpy()\n",
        "predict = MMScaler.inverse_transform(predict)\n",
        "predict = predict[-100:]\n",
        "true = np.squeeze(y_test)\n",
        "true = MMScaler.inverse_transform(true)\n",
        "true = true[-100:]\n",
        "\n",
        "# calculate the daily returns based on the predicted and true prices\n",
        "predict_returns = (predict[:, 1:] - predict[:, :-1]) / predict[:, :-1]\n",
        "true_returns = (true[:, 1:] - true[:, :-1]) / true[:, :-1]\n",
        "\n",
        "# calculate the position you would have taken based on your prediction\n",
        "threshold = 0.00001  # 1% return threshold\n",
        "predict_position = np.where(predict_returns > threshold, 1, -1)\n",
        "true_position = np.where(true_returns > threshold, 1, -1)\n",
        "\n",
        "# define a function to simulate the portfolio based on the predicted or true returns\n",
        "def simulate_portfolio(position, returns, initial_capital):\n",
        "    # calculate the daily profit and loss based on the position you took and the daily returns\n",
        "    transaction_cost = 0.001  # 0.1% per transaction\n",
        "    position_percentage = 0.2  # 10% of capital\n",
        "    pnl = position * (initial_capital * position_percentage) * (returns - transaction_cost)\n",
        "\n",
        "    # calculate the cumulative PnL over the entire period\n",
        "    cumulative_pnl = np.cumsum(pnl)\n",
        "\n",
        "    # calculate the final portfolio value\n",
        "    final_value = initial_capital + cumulative_pnl[-1]\n",
        "\n",
        "    # calculate the profit percentage\n",
        "    profit_percentage = ((final_value - initial_capital) / initial_capital) * 100\n",
        "\n",
        "    return final_value, profit_percentage\n",
        "\n",
        "# simulate the portfolio based on the predicted and true returns for different initial capital values\n",
        "for cap in [1000, 2000, 5000, 10000]:\n",
        "    print(f\"\\nFor initial capital of {cap}:\")\n",
        "    predict_final_value, predict_profit_percentage = simulate_portfolio(predict_position, predict_returns, cap)\n",
        "    true_final_value, true_profit_percentage = simulate_portfolio(true_position, true_returns, cap)\n",
        "    print(f\"Predicted final value   : {predict_final_value:.2f} with {predict_profit_percentage:.2f}% profit\")\n",
        "    print(f\"True final value        : {true_final_value:.2f} with {true_profit_percentage:.2f}% profit\")\n",
        "    print(f\"(Predicted/True)% profit: {predict_final_value*100/true_final_value:.2f}% profit\") \n",
        "    print(\"-----------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZP8DCSDsGLLH"
      },
      "outputs": [],
      "source": [
        "checkpoint = torch.load(f'./weights/model_epoch_{best_epoch}.pth')\n",
        "myModel.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "\n",
        "df_X_ss = SScaler.transform(XX.reshape(-1, 1)) # old transformers\n",
        "df_C_ss = condscaler.transform(Condition)\n",
        "\n",
        "# for i in range(0,len(selectmx.T)):\n",
        "#   col2 = df_C_ss[:,i].copy()\n",
        "#   col2 *= selectmx.T[\"price_usd_close\"][i]/max(selectmx.T[\"price_usd_close\"])\n",
        "#   df_C_ss[:, i] = col2\n",
        "\n",
        "# c_trans = pd.DataFrame(c_trans)\n",
        "# c_trans = c_trans.drop(c_trans.columns[c_trans.apply(lambda col: col.all() == 0)], axis=1)\n",
        "\n",
        "df_y_mm = MMScaler.transform(np.array(data.price_usd_close).reshape(-1, 1))\n",
        "df_y_mm = df_y_mm.squeeze()\n",
        "# split the sequence\n",
        "df_X_ss, df_C_ss, df_y_mm = split_sequences(df_X_ss, df_C_ss, df_y_mm, x_shape, y_shape)\n",
        "# converting to tensors\n",
        "df_X_ss = Variable(torch.Tensor(df_X_ss)) \n",
        "df_C_ss = Variable(torch.Tensor(df_C_ss))\n",
        "df_y_mm = Variable(torch.Tensor(df_y_mm))\n",
        "# reshaping the dataset\n",
        "df_X_ss = torch.reshape(df_X_ss, (df_X_ss.shape[0], x_shape, df_X_ss.shape[2]))\n",
        "df_C_ss = torch.reshape(df_C_ss, (df_C_ss.shape[0], x_shape, df_C_ss.shape[2]))\n",
        "\n",
        "train_predict = myModel(df_X_ss.to(device),df_C_ss.to(device)).cpu() # forward pass\n",
        "\n",
        "data_predict = train_predict.data.numpy() # numpy conversion\n",
        "dataY_plot = df_y_mm.data.numpy()\n",
        "\n",
        "data_predict = MMScaler.inverse_transform(data_predict) # reverse transformation\n",
        "dataY_plot = dataY_plot.squeeze()\n",
        "dataY_plot = MMScaler.inverse_transform(dataY_plot)\n",
        "true, preds = [], []\n",
        "for i in range(len(dataY_plot)):\n",
        "    true.append(dataY_plot[i][0])\n",
        "for i in range(len(data_predict)):\n",
        "    preds.append(data_predict[i][0])\n",
        "\n",
        "\n",
        "plt.figure(figsize=(50,20)) #plotting\n",
        "plt.axvline(x=train_test_cutoff -(x_shape), c='r', linestyle='--') # size of the training set\n",
        "\n",
        "plt.xticks(range(0,total_samples,100))\n",
        "plt.grid(color='g', linestyle=':', linewidth=0.5)\n",
        "plt.plot(true, label='Actual Data') # actual plot\n",
        "plt.plot(preds, label='Predicted Data') # predicted plot\n",
        "plt.title('Time-Series Prediction')\n",
        "plt.legend()\n",
        "# plt.savefig(\"whole_plot.png\", dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkHSrS87GI6Q"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(25,10)) #plotting\n",
        "plt.plot(loss_valid_show, label='ERROR_VALID') \n",
        "plt.plot(loss_train_show, label='ERROR_TRAIN') \n",
        "plt.title('Time-Series Prediction ERROR')\n",
        "plt.grid(color='g', linestyle=':', linewidth=0.5)\n",
        "plt.xticks(range(0,n_epochs+1,30))\n",
        "plt.legend()\n",
        "plt.show()\n",
        "# print(loss_show)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OS_zDp3gg4rh"
      },
      "outputs": [],
      "source": [
        "# myModel\n",
        "# # Print initial weights\n",
        "# print(\"Initial weights:\")\n",
        "# for name, param in myModel.named_parameters():\n",
        "#     if 'weight' in name:\n",
        "#         print(name, param.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMLiQ9AeV5IY"
      },
      "outputs": [],
      "source": [
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir logs\n",
        "\n",
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir /content/drive/MyDrive/logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIO-SwR7V47g"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rw96MOw3EfCq"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'kernel_size_EN': [2,3],\n",
        "    'kernel_size_DE': [2,3],\n",
        "    'hidden_size_lstm': [20],\n",
        "    'num_levels_en': [2,4,6],\n",
        "    'num_levels_de': [2,4,6],\n",
        "    'num_layers_lstm': [6],\n",
        "    'num_layers_gru': [4]\n",
        "}\n",
        "loss_fn = NRMSELoss\n",
        "lr_decay_epoch = 49\n",
        "learning_rate = 0.01\n",
        "weight_decay = 0.001\n",
        "\n",
        "def run_experiment(params):\n",
        "    myModel = TSASeriesNet.ANNmodel(X_train_tensors_final.shape[1], C_train_tensors_final.shape[2], X_train_tensors_final.shape[2],y_train_tensors.shape[1],\n",
        "                                    params['num_levels_en'], params['num_levels_de'], params['kernel_size_EN'],\n",
        "                                    params['kernel_size_DE'], 2, params['hidden_size_lstm'],\n",
        "                                    params['num_layers_lstm'], params['num_layers_gru']).to(device)\n",
        "\n",
        "    optimiser = torch.optim.Adam(myModel.parameters(),weight_decay=0.0001)#, weight_decay=weight_decay, lr=learning_rate)\n",
        "\n",
        "    loss_train_show, loss_valid_show = training_loop(n_epochs=401,\n",
        "                                                      learning_rate=learning_rate,\n",
        "                                                      lr_decay_epoch=lr_decay_epoch,\n",
        "                                                      network=myModel,\n",
        "                                                      optimiser=optimiser,\n",
        "                                                      loss_fn= loss_fn,\n",
        "                                                      X_train=X_train_tensors_final,\n",
        "                                                      Condition_train=C_train_tensors_final,\n",
        "                                                      y_train=y_train_tensors,\n",
        "                                                      X_test=X_test_tensors_final,\n",
        "                                                      Condition_test=C_test_tensors_final,\n",
        "                                                      y_test=y_test_tensors)\n",
        "    return loss_valid_show[-1]\n",
        "\n",
        "import itertools\n",
        "\n",
        "def grid_search(param_grid):\n",
        "    keys, values = zip(*param_grid.items())\n",
        "    min_loss = float('inf')\n",
        "    best_params = None\n",
        "\n",
        "    for v in itertools.product(*values):\n",
        "        params = dict(zip(keys, v))\n",
        "        print(f\"Running experiment with parameters: {params}\")\n",
        "        loss = run_experiment(params)\n",
        "        print(f\"Validation loss: {loss}\")\n",
        "\n",
        "        if loss < min_loss:\n",
        "            min_loss = loss\n",
        "            best_params = params\n",
        "\n",
        "    return best_params, min_loss\n",
        "\n",
        "best_params, min_loss = grid_search(param_grid)\n",
        "print(f\"Best parameters: {best_params}\")\n",
        "print(f\"Minimum validation loss: {min_loss}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xvky0U01GOH5"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5)) #plotting\n",
        "test_predict = myModel(X_test_tensors_final,C_test_tensors_final) # get the last sample\n",
        "test_predict = test_predict.detach().cpu().numpy()\n",
        "test_predict = MMScaler.inverse_transform(test_predict)\n",
        "test_predict = test_predict[-1].tolist()\n",
        "\n",
        "test_target = y_test_tensors.detach().cpu().numpy() # last sample again\n",
        "test_target = MMScaler.inverse_transform(test_target[-1].flatten().reshape(1, -1))\n",
        "test_target = test_target[0].tolist()\n",
        "# plt.xticks(range(0, 14,1))\n",
        "plt.grid(color='g', linestyle='--', linewidth=0.5)\n",
        "plt.plot(test_target, label=\"Actual Data\")\n",
        "plt.plot(test_predict, label=\"Network Predictions\")\n",
        "plt.legend()\n",
        "# plt.savefig(\"small_plot.png\", dpi=300)\n",
        "plt.show();\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gvBidQltMHB"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-CaD7CKRNHr"
      },
      "outputs": [],
      "source": [
        "# !pip install pytorch-model-summary \n",
        "# import pytorch_model_summary as pms\n",
        "# pms.summary(myModel, torch.zeros(X.shape[0], 30, X.shape[1]).to(device),torch.zeros(X.shape[0], 30, X.shape[1]).to(device), show_input=True, print_summary=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}