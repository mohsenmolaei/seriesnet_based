{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohsenmolaei/seriesnet_based/blob/main/seriesNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFy2ZqO3BizP",
        "outputId": "b3853a93-ae0c-4da8-9b63-ea9a69f42392"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/.shortcut-targets-by-id/15DygMAXOJIypb4JGFSiPmdC6GqJjdD9a/tripple stage seriesnet\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd \"/content/drive/MyDrive/Colab Notebooks/tripple stage seriesnet\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNtWkhcB-lhy",
        "outputId": "52df94a9-29c2-444c-8070-c036002791ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "import pdb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder ,StandardScaler\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "# !pip install backtrader\n",
        "\n",
        "# !pip install yfinance\n",
        "import yfinance as yf\n",
        "# !pip install lingam\n",
        "# import feature_selection\n",
        "import TSASeriesNet\n",
        "# import feature_selection\n",
        "if torch.cuda.is_available():\n",
        "    # dev = \"TPU:0\"\n",
        "    dev = \"cuda:0\"\n",
        "else:\n",
        "    dev = \"cpu\"\n",
        "device = torch.device(dev)\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHyrsSKOp01U"
      },
      "outputs": [],
      "source": [
        "data= pd.read_csv('data.csv')\n",
        "data.set_index(\"time_stamp\",inplace=True,drop=True)\n",
        "# data\n",
        "# label_encoder = LabelEncoder()\n",
        "# n_bins = 50\n",
        "# data['difficulty_latest'] = data['difficulty_latest'].astype(float)\n",
        "# data['hash_rate_mean'] = data['hash_rate_mean'].astype(float)\n",
        "\n",
        "# data['difficulty_latest'] = label_encoder.fit_transform(pd.cut(data['difficulty_latest'] , n_bins, retbins=True)[0])\n",
        "# data['hash_rate_mean'] = label_encoder.fit_transform(pd.cut(data['hash_rate_mean'], n_bins, retbins=True)[0])\n",
        "\n",
        "# whatcol=[ 'price_drawdown_relative','difficulty_latest',\n",
        "#            'hash_rate_mean', 'active_more_1y_percent', 'utxo_created_value_median',\n",
        "#            'transfers_volume_median', 'utxo_spent_value_median']\n",
        "\n",
        "# X, Y = data.drop([\"price_usd_close\"],axis =1), np.array(data.price_usd_close) #data[whatcol]\n",
        "# X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zJIM3lXSqgB"
      },
      "outputs": [],
      "source": [
        "# autoscaler = StandardScaler()\n",
        "# features = autoscaler.fit_transform(data)\n",
        "\n",
        "# label_encoder = LabelEncoder()\n",
        "# n_bins = 50\n",
        "# y_train = label_encoder.fit_transform(pd.cut(Y, n_bins, retbins=True)[0])\n",
        "\n",
        "# mi= pd.DataFrame()\n",
        "# mi = mutual_info_classif(X, y_train)#, discrete_features=discrete_vars)\n",
        "# mi = pd.Series(mi)\n",
        "\n",
        "# mi.index = col\n",
        "# mi.sort_values(ascending=False).plot.bar(figsize=(15, 10))\n",
        "# plt.ylabel('Mutual Information')\n",
        "# plt.title(f\"Mutual information between predictors and target.  bins: {n_bins} \")\n",
        "\n",
        "# plt.show()\n",
        "\n",
        "# # xindex = mi.sort_values(ascending=False)[0:8]\n",
        "# # X = data[xindex.index]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXQhvh75p4Qs"
      },
      "outputs": [],
      "source": [
        "data_top15= pd.read_csv('top15.csv')\n",
        "data_top15.set_index(\"time_stamp\",inplace=True,drop=True)\n",
        "\n",
        "# autoscaler = StandardScaler()\n",
        "# features = autoscaler.fit_transform(data_top15)\n",
        "\n",
        "# label_encoder = LabelEncoder()\n",
        "# n_bins = 50\n",
        "\n",
        "# mi15= pd.DataFrame()\n",
        "# mi15 = mutual_info_classif(data_top15, y_train)#, discrete_features=discrete_vars)\n",
        "# mi15 = pd.Series(mi15)\n",
        "\n",
        "# mi15.index = data_top15.columns\n",
        "# mi15.sort_values(ascending=False).plot.bar(figsize=(15, 10))\n",
        "# plt.ylabel('Mutual Information')\n",
        "# plt.title(f\"Mutual information between predictors and target.  bins: {n_bins} \")\n",
        "\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIij7foycRHa"
      },
      "outputs": [],
      "source": [
        "# ccol = ['CL=F', 'CPI', 'DXYN', 'GC=F', 'SI=F', '^DJI', '^IXIC', '^RUT', '^TNX','EURUSD=X', 'GBPUSD=X', '^FTSE', 'N225']\n",
        "\n",
        "Condition= pd.read_csv('Condition.csv')\n",
        "Condition.set_index(\"time_stamp\",inplace=True,drop=True)\n",
        "# Condition\n",
        "\n",
        "# autoscaler = StandardScaler()\n",
        "# features = autoscaler.fit_transform(Condition)\n",
        "\n",
        "# label_encoder = LabelEncoder()\n",
        "# n_bins = 50\n",
        "# y_train = label_encoder.fit_transform(pd.cut(Y, n_bins, retbins=True)[0])\n",
        "\n",
        "# mi= pd.DataFrame()\n",
        "# mi = mutual_info_classif(Condition, y_train)#, discrete_features=discrete_vars)\n",
        "# mi = pd.Series(mi)\n",
        "\n",
        "# mi.index = Condition.columns\n",
        "# mi.sort_values(ascending=False).plot.bar(figsize=(15, 10))\n",
        "# plt.ylabel('Mutual Information')\n",
        "# plt.title(f\"Mutual information between predictors and target.  bins: {n_bins} \")\n",
        "# plt.show()\n",
        "\n",
        "# xindex = mi.sort_values(ascending=False)[0:8]\n",
        "# Condition = data[xindex.index]\n",
        "\n",
        "# Condition.columns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_top15= pd.read_csv('top15_crypto.csv')\n",
        "data_top15.set_index(\"time_stamp\",inplace=True,drop=True)\n",
        "cond = data_top15[:-1]\n",
        "cond = cond.rename(columns={'BTC-USD': 'price_usd_close'})\n",
        "cond = cond.fillna(method='ffill')\n",
        "\n",
        "test_df = cond.iloc[-1000:]\n",
        "xtest = test_df.price_usd_close\n",
        "condtest = test_df.drop(\"price_usd_close\",axis=1)\n",
        "# condtest =test_df#[f_col]\n",
        "ytest = test_df.price_usd_close\n",
        "\n",
        "cond = cond.iloc[:-1000]\n",
        "Y = cond.price_usd_close\n",
        "X = cond.price_usd_close\n",
        "cond = cond.drop(\"price_usd_close\",axis=1)\n",
        "\n",
        "print(Y.shape,X.shape,cond.shape)\n",
        "print(ytest.shape, xtest.shape , condtest.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_54iSrNHI53A",
        "outputId": "fb9acbe0-b953-45ac-e761-1d223c5620b6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16341,) (16341,) (16341, 29)\n",
            "(1000,) (1000,) (1000, 29)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7XXU-uiPMBe"
      },
      "outputs": [],
      "source": [
        "# col = list(data.columns)+ list(Condition.columns)+list(data_top15.columns)\n",
        "# cond = pd.DataFrame(np.hstack((data , Condition)))\n",
        "# cond = pd.DataFrame(np.hstack((cond,data_top15)))\n",
        "# cond.columns =col\n",
        "\n",
        "# f_col = [\"receiving_count\", \"^DJI\"]\n",
        "\n",
        "# import importlib\n",
        "# import feature_selection\n",
        "# importlib.reload(feature_selection)\n",
        "\n",
        "# crypto = \"price_usd_close\"\n",
        "# selectmx , matrix_correlation , matrix_observed , matrix_latent , matrix_granger , matrix_mi = feature_selection.select_matrix(cond, crypto)\n",
        "# Y = cond.price_usd_close\n",
        "# cond =cond.drop([\"price_usd_close\"],axis =1)\n",
        "\n",
        "\n",
        "# selectmx.to_csv(\"select_matrix.csv\")\n",
        "\n",
        "# matrix_correlation.to_csv(\"matrix_correlation.csv\")\n",
        "# matrix_observed.to_csv(\"matrix_observed.csv\")\n",
        "# matrix_latent.to_csv(\"matrix_latent.csv\")\n",
        "# matrix_granger.to_csv(\"matrix_granger.csv\")\n",
        "# matrix_mi.to_csv(\"matrix_mi.csv\")\n",
        "\n",
        "\n",
        "# داده های تست روزهای اخیر\n",
        "# test_data= pd.read_csv('test_data.csv')\n",
        "# xtest = test_data.price_usd_close\n",
        "# condtest = test_data[[\"receiving_count\", \"^DJI\"]]\n",
        "# ytest = test_data.price_usd_close\n",
        "\n",
        "\n",
        "# test_df = cond.iloc[-200:]\n",
        "# xtest = test_df.price_usd_close\n",
        "# condtest = test_df.drop(\"price_usd_close\",axis=1)\n",
        "# # condtest =condtest#[f_col]\n",
        "# ytest = test_df.price_usd_close\n",
        "\n",
        "# cond = cond.iloc[:-200]\n",
        "# Y = cond.price_usd_close\n",
        "# X = cond.price_usd_close\n",
        "# cond = cond.drop(\"price_usd_close\",axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgUp_ziFe8DU"
      },
      "outputs": [],
      "source": [
        "# start = \"2006-01-01\"\n",
        "# end=\"2023-3-28\"\n",
        "# indnas = [\"^IXIC\", \"AAPL\", \"MSFT\", \"AMZN\", \"GOOGL\", \"TSLA\", \"NVDA\", \"PYPL\", \"ADBE\", \"NFLX\", \"INTC\", \"CSCO\", \"CMCSA\", \"AMD\", \"PEP\", \"CHTR\", \"TMUS\", \"AVGO\", \"GILD\", \"AMGN\", \"QCOM\", \"JD\", \"TXN\", \"BIIB\", \"VRTX\", \"REGN\", \"ISRG\", \"MRNA\", \"COST\", \"BKNG\", \"ADP\"]\n",
        "# try :\n",
        "#     data_top_index = yf.download(indnas ,start=start, end=end , interval =\"1d\") #1137\n",
        "# except Exception as e:\n",
        "#     print(\"error\")\n",
        "# data_top_index.set_index(pd.to_datetime(data_top_index.index).date,inplace=True)\n",
        "# data_top_index = data_top_index.Close.fillna(method='ffill')\n",
        "\n",
        "# try:\n",
        "#     data_crypto = yf.download(\"BTC-USD\" ,start=start, end=end , interval =\"1d\")\n",
        "# except Exception as e:\n",
        "#     print(\"error\")\n",
        "# data_crypto.set_index(pd.to_datetime(data_crypto.index).date,inplace=True)\n",
        "\n",
        "# merged_df = pd.merge(data_crypto.Close, data_top_index , left_index=True, right_index=True, how='outer')\n",
        "# merged_df.fillna(method='bfill', inplace=True)\n",
        "\n",
        "# merged_df = merged_df.rename_axis('time_stamp')\n",
        "\n",
        "# merged_df.to_csv(\"nasdaq.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JfzFgtZq8KH"
      },
      "outputs": [],
      "source": [
        "# داده های مقاله\n",
        "data_top_index= pd.read_csv('nasdaq100_padding.csv')\n",
        "\n",
        "test_df = data_top_index.iloc[-8180:] #8180\n",
        "cond = data_top_index.iloc[:-8180]\n",
        "Y = cond[\"NDX\"]\n",
        "X = cond[\"NDX\"]\n",
        "# cond = cond.drop(\"NVDA\",axis=1)\n",
        "cond = cond.drop(\"NDX\",axis=1)\n",
        "\n",
        "xtest = test_df[\"NDX\"]\n",
        "ytest = test_df[\"NDX\"]\n",
        "condtest = test_df.drop(\"NDX\",axis=1)\n",
        "# condtest = condtest.drop(\"NDX\",axis=1)\n",
        "\n",
        "f_col = col= condtest.columns.values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rKRQ00E5SMN"
      },
      "outputs": [],
      "source": [
        "# # start = \"2023-04-01\"\n",
        "# # end=\"2023-5-09\"\n",
        "# data_top_index= pd.read_csv('nasdaq.csv')\n",
        "# data_top_index.set_index(\"time_stamp\",inplace=True,drop=True)\n",
        "# # data_top_index = data_top_index[['^IXIC','AAPL','GOOGL','AMZN', 'MRVL', 'MSFT',]]\n",
        "# # Y = data_top_index[\"^IXIC\"]\n",
        "# # data_top_index = data_top_index.drop(\"^IXIC\",axis =1)\n",
        "\n",
        "# # autoscaler = StandardScaler()\n",
        "# # features = autoscaler.fit_transform(data_top_index)\n",
        "\n",
        "# # label_encoder = LabelEncoder()\n",
        "# # n_bins = 50\n",
        "# # y_train = label_encoder.fit_transform(pd.cut(Y, n_bins, retbins=True)[0])\n",
        "\n",
        "# # mi= pd.DataFrame()\n",
        "# # mi = mutual_info_classif(data_top_index, y_train)#, discrete_features=discrete_vars)\n",
        "# # mi = pd.Series(mi)\n",
        "\n",
        "# # mi.index = data_top_index.columns\n",
        "# # mi.sort_values(ascending=False).plot.bar(figsize=(15, 10))\n",
        "# # plt.ylabel('Mutual Information')\n",
        "# # plt.title(f\"Mutual information between predictors and target.  bins: {n_bins} \")\n",
        "# # plt.show()\n",
        "# test_df = data_top_index.iloc[-430:]\n",
        "# cond = data_top_index.iloc[:-430]\n",
        "# Y = cond[\"AAPL\"]\n",
        "# X = cond[\"^IXIC\"]\n",
        "# cond = cond.drop(\"AAPL\",axis=1)\n",
        "# cond = cond.drop(\"^IXIC\",axis=1)\n",
        "\n",
        "# xtest = test_df[\"^IXIC\"]\n",
        "# ytest = test_df[\"AAPL\"]\n",
        "# condtest = test_df.drop(\"AAPL\",axis=1)\n",
        "# condtest = condtest.drop(\"^IXIC\",axis=1)\n",
        "\n",
        "# f_col = col= condtest.columns.values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wdu0H41jVrLl",
        "outputId": "9c5e8ae7-6faa-40b8-a64f-4bd5bcd91f9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32380     0.000000\n",
            "32381     0.000000\n",
            "32382     0.000000\n",
            "32383     4.363515\n",
            "32384    27.517971\n",
            "           ...    \n",
            "40555    64.311538\n",
            "40556    60.550920\n",
            "40557    63.768070\n",
            "40558    72.750859\n",
            "40559    72.614326\n",
            "Name: NDX, Length: 8180, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def calculate_rsi(data, window_length):\n",
        "    # Calculate the price differences\n",
        "    delta = data.diff()\n",
        "\n",
        "    # Make the positive gains (up) and negative gains (down) Series\n",
        "    up, down = delta.copy(), delta.copy()\n",
        "    up[up < 0] = 0\n",
        "    down[down > 0] = 0\n",
        "\n",
        "    # Calculate the EWMA (Exponential Weighted Moving Average) of the gains and losses\n",
        "    gain_avg = up.ewm(span=window_length).mean()\n",
        "    loss_avg = down.abs().ewm(span=window_length).mean()\n",
        "\n",
        "    # Calculate the Relative Strength (RS)\n",
        "    rs = gain_avg / loss_avg\n",
        "\n",
        "    # Calculate the RSI\n",
        "    rsi = 100.0 - (100.0 / (1.0 + rs))\n",
        "\n",
        "    return rsi\n",
        "\n",
        "# Calculate the RSI with a window length of 14 days\n",
        "X = calculate_rsi(X, window_length=14)\n",
        "X.fillna(method='bfill', inplace=True)\n",
        "\n",
        "xtest = calculate_rsi(xtest, window_length=14)\n",
        "xtest.fillna(method='bfill', inplace=True)\n",
        "\n",
        "# Print the RSI values\n",
        "print(xtest)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I62jK5lXV0vj"
      },
      "source": [
        "**TEST data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiC709vaog2Z",
        "outputId": "ee5c2cc3-866c-4621-fae0-8263a03d98d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16341, 1)\n",
            "(16341, 29)\n",
            "(16341, 1)\n",
            "(16274, 60, 1) (16274, 60, 29) (16274, 7, 1)\n",
            "3268\n",
            "Training Shape: (13006, 60, 1) (13006, 7, 1)\n",
            "Validatin Shape: (3268, 60, 1) (3268, 7, 1)\n",
            "torch.Size([13006, 60, 1])\n",
            "torch.Size([13006, 60, 29])\n",
            "Training Shape: torch.Size([13006, 60, 1]) torch.Size([13006, 60, 29]) torch.Size([13006, 7, 1])\n",
            "Validation Shape: torch.Size([3268, 60, 1]) torch.Size([3268, 60, 29]) torch.Size([3268, 7, 1])\n",
            "Test Shape: torch.Size([933, 60, 1]) torch.Size([933, 60, 29]) torch.Size([933, 7, 1])\n"
          ]
        }
      ],
      "source": [
        "\"\"\"# scale features\"\"\"\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "MMScaler = MinMaxScaler()\n",
        "SScaler = StandardScaler()\n",
        "condscaler = StandardScaler()\n",
        "\n",
        "MMScaler_test = MinMaxScaler()\n",
        "SScaler_test = StandardScaler()\n",
        "condscaler_test = StandardScaler()\n",
        "\n",
        "# train and validation data scaler\n",
        "X_trans = SScaler.fit_transform(pd.DataFrame(np.array(Y).reshape(-1, 1)))#Y.reshape(-1, 1))\n",
        "condtemp = cond\n",
        "c_trans = condscaler.fit_transform(condtemp)#[f_col])\n",
        "\n",
        "# for i in range(0,len(selectmx.T)):\n",
        "#   col2 = c_trans[:,i].copy()\n",
        "#   if selectmx.T[\"price_usd_close\"][i] >= max(selectmx.T[\"price_usd_close\"])  :\n",
        "#     print(col[i])\n",
        "#     col2 *= selectmx.T[\"price_usd_close\"][i]/max(selectmx.T[\"price_usd_close\"])\n",
        "#   else:\n",
        "#     col2 *= 0\n",
        "# #   c_trans[:, i] = col2\n",
        "# print(\"\\n\\n\")\n",
        "\n",
        "# for i in range(0,len(result_df.T)):\n",
        "#   col2 = c_trans[:,i].copy()\n",
        "#   if result_df.squeeze().tolist()[i] >= 1:\n",
        "#     print(col[i])\n",
        "#     col2 *= result_df.T[i]\n",
        "#   else:\n",
        "#     col2 *= 0\n",
        "#   c_trans[:, i] = col2\n",
        "# print(\"\\n\\n\")\n",
        "\n",
        "# c_trans = pd.DataFrame(c_trans)\n",
        "# c_trans = c_trans.drop(c_trans.columns[c_trans.apply(lambda col: col2.all() == 0)], axis=1)\n",
        "\n",
        "\n",
        "# for i in range(0,len(selectmx.T)):\n",
        "#   col2 = c_trans[:,i].copy()\n",
        "#   if selectmx.T[\"Close\"][i] >= max(selectmx.T[\"Close\"]) -4:\n",
        "#     print(col[i])\n",
        "#     col2 *= selectmx.T[\"Close\"][i]/max(selectmx.T[\"Close\"])\n",
        "#   else:\n",
        "#     col2 *= 0\n",
        "#   c_trans[:, i] = col2\n",
        "# print(\"\\n\\n\")\n",
        "# c_trans = pd.DataFrame(c_trans)\n",
        "# c_trans = c_trans.drop(c_trans.columns[c_trans.apply(lambda col: col.all() == 0)], axis=1)\n",
        "\n",
        "\n",
        "y_trans = MMScaler.fit_transform(pd.DataFrame(np.array(Y).reshape(-1, 1)))\n",
        "print(X_trans.shape)\n",
        "print(c_trans.shape)\n",
        "print(y_trans.shape)\n",
        "\n",
        "# Test data Scaler\n",
        "X_trans_test = SScaler_test.fit_transform(xtest.values.reshape(-1, 1))\n",
        "c_trans_test = condscaler_test.fit_transform(condtest)\n",
        "y_trans_test = MMScaler_test.fit_transform(ytest.values.reshape(-1, 1))\n",
        "\n",
        "\"\"\"# split a multivariate sequence past, future samples (X and y)\"\"\"\n",
        "def split_sequences(input_sequences, condition_seq, output_sequence, n_steps_in, n_steps_out):\n",
        "    X, C, y = list(),list(), list()\n",
        "    for i in range(len(input_sequences)):\n",
        "        end_ix = i + n_steps_in\n",
        "        out_end_ix = end_ix + n_steps_out\n",
        "        if out_end_ix >= len(input_sequences): break\n",
        "        seq_x, seq_c, seq_y = input_sequences[i:end_ix], condition_seq[i:end_ix], output_sequence[end_ix:out_end_ix]\n",
        "        X.append(seq_x), C.append(seq_c) ,y.append(seq_y)\n",
        "    return np.array(X), np.array(C), np.array(y)\n",
        "\n",
        "\n",
        "x_shape = 60\n",
        "y_shape = 7\n",
        "# Train data\n",
        "X_ss, C_ss, y_mm = split_sequences(X_trans, c_trans, y_trans, x_shape, y_shape)\n",
        "print(X_ss.shape, C_ss.shape, y_mm.shape)\n",
        "\n",
        "#Test data\n",
        "DataX_test, DataC_test, Datay_test = split_sequences(X_trans_test, c_trans_test, y_trans_test, x_shape, y_shape)\n",
        "\n",
        "total_samples = len(cond)\n",
        "train_test_cutoff = round(0.8 * total_samples) # 0.8\n",
        "\n",
        "X_train = X_ss[:-(total_samples-train_test_cutoff)]\n",
        "C_train = C_ss[:-(total_samples-train_test_cutoff)]\n",
        "X_test = X_ss[-(total_samples-train_test_cutoff):]\n",
        "C_test = C_ss[-(total_samples-train_test_cutoff):]\n",
        "\n",
        "y_train = y_mm[:-(total_samples-train_test_cutoff)]\n",
        "y_test = y_mm[-(total_samples-train_test_cutoff):]\n",
        "\n",
        "# Randomly shuffle the indices\n",
        "indices = np.arange(len(X_train))\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "# Reorder the data using the shuffled indices\n",
        "X_train = X_train[indices]\n",
        "C_train = C_train[indices]\n",
        "y_train = y_train[indices]\n",
        "\n",
        "print(total_samples - train_test_cutoff)\n",
        "print(\"Training Shape:\", X_train.shape, y_train.shape)\n",
        "print(\"Validatin Shape:\", X_test.shape, y_test.shape)\n",
        "\n",
        "\"\"\"# convert to pytorch tensors\"\"\"\n",
        "X_train_tensors = Variable(torch.Tensor(X_train).to(device))\n",
        "X_test_tensors = Variable(torch.Tensor(X_test).to(device))\n",
        "\n",
        "C_train_tensors = Variable(torch.Tensor(C_train).to(device))\n",
        "C_test_tensors = Variable(torch.Tensor(C_test).to(device))\n",
        "\n",
        "y_train_tensors = Variable(torch.Tensor(y_train).to(device))\n",
        "y_test_tensors = Variable(torch.Tensor(y_test).to(device))\n",
        "\n",
        "print(X_train_tensors.shape)\n",
        "print(C_train_tensors.shape)\n",
        "\n",
        "# DataX_test, DataC_test, Datay_test\n",
        "DataX_test = Variable(torch.Tensor(DataX_test).to(device))\n",
        "DataC_test = Variable(torch.Tensor(DataC_test).to(device))\n",
        "Datay_test = Variable(torch.Tensor(Datay_test).to(device))\n",
        "\n",
        "\"\"\"# reshaping to rows, timestamps, features\"\"\"\n",
        "\n",
        "X_train_tensors_final = torch.reshape(X_train_tensors,\n",
        "                                      (X_train_tensors.shape[0], x_shape,\n",
        "                                       X_train_tensors.shape[2]))\n",
        "X_test_tensors_final = torch.reshape(X_test_tensors,\n",
        "                                     (X_test_tensors.shape[0], x_shape,\n",
        "                                      X_test_tensors.shape[2]))\n",
        "\n",
        "C_train_tensors_final = torch.reshape(C_train_tensors,\n",
        "                                      (C_train_tensors.shape[0], x_shape,\n",
        "                                       C_train_tensors.shape[2]))\n",
        "C_test_tensors_final = torch.reshape(C_test_tensors,\n",
        "                                     (C_test_tensors.shape[0], x_shape,\n",
        "                                      C_test_tensors.shape[2]))\n",
        "\n",
        "print(\"Training Shape:\", X_train_tensors_final.shape, C_train_tensors_final.shape, y_train_tensors.shape)\n",
        "print(\"Validation Shape:\", X_test_tensors_final.shape, C_test_tensors_final.shape, y_test_tensors.shape)\n",
        "print(\"Test Shape:\", DataX_test.shape, DataC_test.shape, Datay_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ggEJ_DwqBnm0"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "def NRMSELoss(yhat,y): #NRMSE\n",
        "  y = y.reshape(y.shape[0], y.shape[1])\n",
        "  return (torch.sqrt(torch.mean(torch.square(yhat-y))))/ (torch.max(y) - torch.min(y))\n",
        "\n",
        "# Root Mean Squared Percentage Error (RMSPE)\n",
        "def rmspe(y_pred,y_true):\n",
        "  y_true = y_true.reshape(y_true.shape[0], y_true.shape[1])\n",
        "  return torch.sqrt(torch.mean(torch.square((y_true - y_pred) / y_true)))\n",
        "\n",
        "def MAPELoss(yhat,y):\n",
        "  y = y.reshape(y.shape[0], y.shape[1])\n",
        "  return torch.mean(torch.abs(yhat-y)/torch.abs(y))\n",
        "\n",
        "def SMAPELoss(y_pred, y_true):\n",
        "  y_true = y_true.reshape(y_true.shape[0], y_true.shape[1])\n",
        "  loss = 2 * torch.mean(torch.abs(y_true - y_pred) / (torch.max(y_true) + torch.max(y_true)))\n",
        "  return loss\n",
        "\n",
        "def MSELoss(y_pred, y_true):\n",
        "  y_true = y_true.reshape(y_true.shape[0], y_true.shape[1])\n",
        "  loss = torch.mean((y_pred - y_true)**2)\n",
        "  return loss\n",
        "\n",
        "def RMSELoss(y_pred, y_true):\n",
        "  y_true = y_true.reshape(y_true.shape[0], y_true.shape[1])\n",
        "  loss = torch.sqrt(torch.mean((y_pred - y_true)**2))\n",
        "  return loss\n",
        "\n",
        "def RMSLELoss(y_pred, y_true):\n",
        "  y_true = y_true.reshape(y_true.shape[0], y_true.shape[1])\n",
        "  loss = torch.sqrt(torch.mean((torch.log(y_pred+1) - torch.log(y_true+1))**2))\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hWEL0qCuHuYw"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorboard\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "# writer = SummaryWriter('logs')\n",
        "\n",
        "# def exp_lr_scheduler(optimizer, epoch, init_lr=0.01, lr_decay_epoch=200):\n",
        "#     lr = init_lr * (0.9**(epoch // lr_decay_epoch))\n",
        "#     for param_group in optimizer.param_groups:\n",
        "#         param_group['lr'] = lr\n",
        "#     return optimizer\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "def training_loop(n_epochs, learning_rate, lr_decay_epoch, network, optimiser, loss_fn, X_train, Condition_train, y_train, X_test, Condition_test, y_test, batch_size):\n",
        "    loss_valid_show, loss_train_show = [], []\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        network.cuda()\n",
        "\n",
        "    best_valid_loss = float('inf')\n",
        "    best_epoch = -1\n",
        "\n",
        "    # Split data into batches\n",
        "    train_loader = DataLoader(TensorDataset(X_train, Condition_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(TensorDataset(X_test, Condition_test, y_test), batch_size=batch_size)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        network.train()\n",
        "        train_loss = 0.0\n",
        "        for i, (X_batch, Condition_batch, y_batch) in enumerate(train_loader):\n",
        "            optimiser.zero_grad()\n",
        "            outputs = network.forward(X_batch, Condition_batch)\n",
        "            loss = loss_fn(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimiser.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Calculate average training loss across batches\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        network.eval()\n",
        "        test_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for i, (X_batch, Condition_batch, y_batch) in enumerate(test_loader):\n",
        "                outputs = network.forward(X_batch, Condition_batch)\n",
        "                loss = loss_fn(outputs, y_batch)\n",
        "                test_loss += loss.item()\n",
        "\n",
        "        # Calculate average testing loss across batches\n",
        "        test_loss /= len(test_loader)\n",
        "\n",
        "        loss_valid_show.append(test_loss)\n",
        "        loss_train_show.append(train_loss)\n",
        "\n",
        "\n",
        "        if test_loss < best_valid_loss:\n",
        "            best_valid_loss = test_loss\n",
        "            best_epoch = epoch\n",
        "            torch.save({'epoch': epoch, 'state_dict': network.state_dict()}, f'./weights/model_epoch_{epoch}.pth')\n",
        "\n",
        "        if (epoch) % (5) == 0:\n",
        "            print(\"Epoch: %4d, Train loss: %1.5f, Validation loss: %1.5f\" % (epoch, train_loss, test_loss))\n",
        "\n",
        "    # writer.close()\n",
        "    return loss_train_show, loss_valid_show, best_valid_loss, best_epoch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAuPumvpEzYG",
        "outputId": "51ce7915-f2ba-4388-c3da-b7af91bb2f38"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "124"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ],
      "source": [
        "\"\"\"# Training\"\"\"\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qV8DvLtJFSfr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dd3325a-e502-4ef8-ba2d-264a95819814"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:    0, Train loss: 0.35773, Validation loss: 0.15584\n",
            "Epoch:    5, Train loss: 0.20866, Validation loss: 0.13709\n",
            "Epoch:   10, Train loss: 0.19673, Validation loss: 0.11783\n",
            "Epoch:   15, Train loss: 0.11490, Validation loss: 0.07917\n",
            "Epoch:   20, Train loss: 0.07775, Validation loss: 0.05127\n",
            "Epoch:   25, Train loss: 0.06759, Validation loss: 0.03519\n",
            "Epoch:   30, Train loss: 0.06096, Validation loss: 0.02435\n",
            "Epoch:   35, Train loss: 0.05651, Validation loss: 0.01582\n",
            "Epoch:   40, Train loss: 0.05272, Validation loss: 0.01141\n",
            "Epoch:   45, Train loss: 0.05002, Validation loss: 0.01065\n",
            "Epoch:   50, Train loss: 0.04756, Validation loss: 0.00899\n",
            "Epoch:   55, Train loss: 0.04593, Validation loss: 0.00882\n",
            "Epoch:   60, Train loss: 0.04441, Validation loss: 0.00939\n"
          ]
        }
      ],
      "source": [
        "# torch.manual_seed(17)\n",
        "# torch.cuda.manual_seed(17)\n",
        "# np.random.seed(17)\n",
        "# torch.backends.cudnn.deterministic=True\n",
        "\n",
        "import importlib\n",
        "import Decoder_DARLM\n",
        "import Encoder_DARLM\n",
        "import ts_lstm\n",
        "import ts_gru\n",
        "import HSAM\n",
        "import CBAM\n",
        "# import lstm_gru_model\n",
        "# import sfm\n",
        "# import sfm_test\n",
        "\n",
        "importlib.reload(CBAM)\n",
        "importlib.reload(Decoder_DARLM)\n",
        "importlib.reload(TSASeriesNet)\n",
        "importlib.reload(Encoder_DARLM)\n",
        "importlib.reload(ts_lstm)\n",
        "importlib.reload(ts_gru)\n",
        "importlib.reload(HSAM)\n",
        "# importlib.reload(lstm_gru_model)\n",
        "# importlib.reload(sfm)\n",
        "# importlib.reload(sfm_test)\n",
        "n_epochs = 121\n",
        "num_inputs = X_train_tensors_final.shape[1]\n",
        "dilation_c = 2\n",
        "kernel_size_EN = 2\n",
        "kernel_size_DE = 2\n",
        "hidden_size_lstm =10\n",
        "\n",
        "num_levels_en = 4\n",
        "num_levels_de = 4\n",
        "num_layers_lstm = 3\n",
        "num_layers_gru = 3\n",
        "features = X_train_tensors_final.shape[2]\n",
        "features_c = C_train_tensors_final.shape[2]\n",
        "output_num = y_train_tensors.shape[1]\n",
        "\n",
        "lr_decay_epoch = 49\n",
        "learning_rate = 0.001\n",
        "weight_decay = 0.001\n",
        "batch_size = 2000\n",
        "loss_fn = RMSELoss #SMAPELoss #MAPELoss #NRMSELoss #rmspe #RMSELoss\n",
        "# ANN model\n",
        "myModel = TSASeriesNet.ANNmodel(num_inputs, features_c, features, output_num, num_levels_en,num_levels_de, kernel_size_EN, kernel_size_DE, dilation_c, hidden_size_lstm, num_layers_lstm, num_layers_gru ).to(device)\n",
        "optimiser = torch.optim.Adam(myModel.parameters(),weight_decay=weight_decay)#, lr=learning_rate)#, eps=1e-08)#\n",
        "\n",
        "loss_train_show ,loss_valid_show , best_valid_loss , best_epoch= training_loop(n_epochs=n_epochs,\n",
        "                                  learning_rate = learning_rate,\n",
        "                                  lr_decay_epoch = lr_decay_epoch,\n",
        "                                  network=myModel,\n",
        "                                  optimiser=optimiser,\n",
        "                                  loss_fn=loss_fn,\n",
        "                                  X_train=X_train_tensors_final,\n",
        "                                  Condition_train = C_train_tensors_final,\n",
        "                                  y_train=y_train_tensors,\n",
        "                                  X_test=X_test_tensors_final,\n",
        "                                  Condition_test  = C_test_tensors_final,\n",
        "                                  y_test=y_test_tensors,\n",
        "                                  batch_size = batch_size)\n",
        "\n",
        "print(f\"best validation loss: {best_valid_loss} in epoch {best_epoch}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OS_zDp3gg4rh"
      },
      "outputs": [],
      "source": [
        "checkpoint = torch.load(f'./weights/model_epoch_{best_epoch}.pth', map_location=torch.device('cpu'))\n",
        "myModel.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "for i in range(num_levels_en):\n",
        "    weights = pd.DataFrame(myModel.en_darlm.network[i].pointwise_conv.weight.cpu().detach().numpy().reshape(myModel.en_darlm.network[i].pointwise_conv.weight.size(0), -1))\n",
        "    row_means = np.mean((weights), axis=1)\n",
        "    row_means_df = pd.DataFrame(row_means)\n",
        "    row_means_df.index = col\n",
        "    result_df = np.tanh(row_means_df) #row_means_df.applymap(lambda x: 1 if x > row_means_df[0].mean() else 0 if x == row_means_df[0].mean() else 0)\n",
        "    # print(result_df)\n",
        "    result_df.to_csv(f\"causal/pointwise_conv_weights_mean{i}.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbESekmfAFXR"
      },
      "outputs": [],
      "source": [
        "checkpoint = torch.load(f'./weights/model_epoch_{best_epoch}.pth')\n",
        "myModel.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "# with torch.no_grad():\n",
        "#   test_predict = myModel(DataX_test, DataC_test)  # get the last sample\n",
        "# test_predict = test_predict.detach().cpu().numpy()\n",
        "# test_predict = MMScaler_test.inverse_transform(test_predict)\n",
        "\n",
        "# test_target = Datay_test.detach().cpu().numpy()  # last sample again\n",
        "\n",
        "with torch.no_grad():\n",
        "  test_predict = myModel(DataX_test, DataC_test)  # get the last sample\n",
        "test_predict = test_predict.detach().cpu().numpy()\n",
        "\n",
        "test_target = Datay_test.detach().cpu().numpy()  # last sample again\n",
        "\n",
        "def NRMSELoss_test(yhat,y): #NRMSE\n",
        "  return np.sqrt(np.mean(np.square(yhat-y)))\n",
        "\n",
        "# NRMSELoss_res = NRMSELoss_test(test_predict[:,0] ,first_day_test_target.reshape(-1) )\n",
        "NRMSELoss_res = NRMSELoss_test(test_predict ,test_target.reshape(test_target.shape[0],test_target.shape[1]) )\n",
        "print(NRMSELoss_res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZP8DCSDsGLLH"
      },
      "outputs": [],
      "source": [
        "checkpoint = torch.load(f'./weights/model_epoch_{best_epoch}.pth')\n",
        "myModel.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "\n",
        "df_X_ss = SScaler.transform(pd.DataFrame(np.array(Y).reshape(-1, 1))) # old transformers\n",
        "\n",
        "df_C_ss = condscaler.transform(condtemp)#[f_col])\n",
        "\n",
        "\n",
        "df_y_mm = MMScaler.transform(pd.DataFrame(np.array(Y).reshape(-1, 1)))\n",
        "df_y_mm = df_y_mm.squeeze()\n",
        "# split the sequence\n",
        "df_X_ss, df_C_ss, df_y_mm = split_sequences(df_X_ss, df_C_ss, df_y_mm, x_shape, y_shape)\n",
        "# converting to tensors\n",
        "df_X_ss = Variable(torch.Tensor(df_X_ss))\n",
        "df_C_ss = Variable(torch.Tensor(df_C_ss))\n",
        "df_y_mm = Variable(torch.Tensor(df_y_mm))\n",
        "# reshaping the dataset\n",
        "df_X_ss = torch.reshape(df_X_ss, (df_X_ss.shape[0], x_shape, df_X_ss.shape[2]))\n",
        "df_C_ss = torch.reshape(df_C_ss, (df_C_ss.shape[0], x_shape, df_C_ss.shape[2]))\n",
        "\n",
        "# train_predict = myModel(df_X_ss.to(device),df_C_ss.to(device)).cpu() # forward pass\n",
        "\n",
        "# data_predict = train_predict.data.numpy() # numpy conversion\n",
        "# dataY_plot = df_y_mm.data.numpy()\n",
        "\n",
        "# data_predict = MMScaler.inverse_transform(data_predict) # reverse transformation\n",
        "# dataY_plot = dataY_plot.squeeze()\n",
        "# dataY_plot = MMScaler.inverse_transform(dataY_plot)\n",
        "# true, preds = [], []\n",
        "# for i in range(len(dataY_plot)):\n",
        "#     true.append(dataY_plot[i][0])\n",
        "# for i in range(len(data_predict)):\n",
        "#     preds.append(data_predict[i][0])\n",
        "\n",
        "\n",
        "# plt.figure(figsize=(30,15)) #plotting\n",
        "# plt.axvline(x=train_test_cutoff -(x_shape), c='r', linestyle='--') # size of the training set\n",
        "\n",
        "# plt.xticks(range(0,total_samples,100))\n",
        "# plt.grid(color='g', linestyle=':', linewidth=0.5)\n",
        "# plt.plot(true, label='Actual Data') # actual plot\n",
        "# plt.plot(preds, label='Predicted Data') # predicted plot\n",
        "# plt.title('Time-Series Prediction')\n",
        "# plt.legend()\n",
        "# # plt.savefig(\"whole_plot.png\", dpi=300)\n",
        "# plt.show()\n",
        "\n",
        "predict_loader = DataLoader(TensorDataset(df_X_ss, df_C_ss), batch_size=1000)\n",
        "\n",
        "# make predictions for each batch and concatenate the results\n",
        "predictions = []\n",
        "i=0\n",
        "with torch.no_grad():\n",
        "  for X_batch, C_batch in predict_loader:\n",
        "      # print(i)\n",
        "      # pdb.set_trace()\n",
        "      # i+=1\n",
        "      prediction_batch = myModel(X_batch.to(device), C_batch.to(device)).cpu()\n",
        "      predictions.append(prediction_batch)\n",
        "predictions = torch.cat(predictions)\n",
        "\n",
        "# convert predictions and ground truth to numpy arrays\n",
        "predicted_values = predictions.data.numpy().squeeze()\n",
        "ground_truth = df_y_mm.data.numpy().squeeze()\n",
        "\n",
        "# inverse transform the data to get the original scale\n",
        "predicted_values = MMScaler.inverse_transform(predicted_values)\n",
        "ground_truth = MMScaler.inverse_transform(ground_truth)\n",
        "\n",
        "true, preds = [], []\n",
        "for i in range(len(ground_truth)):\n",
        "    true.append(ground_truth[i][0])\n",
        "for i in range(len(predicted_values)):\n",
        "    preds.append(predicted_values[i][0])\n",
        "\n",
        "\n",
        "plt.figure(figsize=(100,50)) #plotting\n",
        "plt.axvline(x=train_test_cutoff -(x_shape), c='r', linestyle='--') # size of the training set\n",
        "\n",
        "# plot the results\n",
        "plt.xticks(range(0,total_samples,100))\n",
        "plt.grid(color='g', linestyle=':', linewidth=0.5)\n",
        "plt.plot(true, label='Actual Data') # actual plot\n",
        "plt.plot(preds, label='Predicted Data') # predicted plot\n",
        "plt.title('Time-Series Prediction')\n",
        "plt.legend()\n",
        "# plt.savefig(\"whole_plot.png\", dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkHSrS87GI6Q"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(25,10)) #plotting\n",
        "plt.plot(loss_valid_show, label='ERROR_VALID')\n",
        "plt.plot(loss_train_show, label='ERROR_TRAIN')\n",
        "plt.title('Time-Series Prediction ERROR')\n",
        "plt.grid(color='g', linestyle=':', linewidth=0.5)\n",
        "plt.xticks(range(0,n_epochs+1,5))\n",
        "plt.legend()\n",
        "plt.show()\n",
        "# print(loss_show)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xvky0U01GOH5"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5)) #plotting\n",
        "with torch.no_grad():\n",
        "  test_predict = myModel(DataX_test, DataC_test)  # get the last sample\n",
        "test_predict = test_predict.detach().cpu().numpy()\n",
        "test_predict = MMScaler_test.inverse_transform(test_predict)\n",
        "first_day_test_predict = test_predict[:, 0]  # get the first day of the 7-day predictions\n",
        "\n",
        "test_target = Datay_test.detach().cpu().numpy()  # last sample again\n",
        "test_target = MMScaler_test.inverse_transform(test_target.flatten().reshape(1, -1))\n",
        "first_day_test_target = test_target[:, ::y_shape]  # get the corresponding first day in true price\n",
        "\n",
        "plt.grid(color='g', linestyle='--', linewidth=0.5)\n",
        "plt.plot(first_day_test_target[0], label=\"Actual Data\")\n",
        "plt.plot(first_day_test_predict, label=\"Network Predictions\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yl9IFpos558Z"
      },
      "outputs": [],
      "source": [
        "from pytz import timezone\n",
        "def reshape_df(df,l=90):\n",
        "    a = list()\n",
        "    for i in range(len(df)-l+1):\n",
        "        seq_x = df[i:l+i]\n",
        "        a.append(seq_x)\n",
        "    print(np.array(a).shape)\n",
        "    return a\n",
        "\n",
        "x_len=90\n",
        "from datetime import datetime, timedelta\n",
        "end=datetime.now()- timedelta(hours=1) #timedelta(hours=1 , minutes=2)\n",
        "start = end - timedelta(hours=x_len+5)\n",
        "\n",
        "try :\n",
        "    top15_test = yf.download(\"ADA-USD AVAX-USD BNB-USD BTC-USD DOGE-USD DOT-USD ETH-USD LTC-USD MATIC-USD SHIB-USD SOL-USD TRX-USD USDC-USD USDT-USD XRP-USD\" ,start=start, end=end , interval =\"1h\") #1137\n",
        "except Exception as e:\n",
        "    print(\"error\")\n",
        "\n",
        "# top15_test[('Adj Close',)]\n",
        "top15_test = top15_test.drop([('High',)],axis=1)\n",
        "top15_test = top15_test.drop([('Low',)],axis=1)\n",
        "top15_test = top15_test.drop([('Open',)],axis=1)\n",
        "top15_test = top15_test.drop([('Adj Close',)],axis=1)\n",
        "\n",
        "top15_test = top15_test.rename_axis('time_stamp')\n",
        "top15_test = top15_test.replace(0, np.nan)#.fillna(method='ffill')\n",
        "top15_test = top15_test.fillna(method='bfill')\n",
        "\n",
        "top15_close = top15_test[('Close',)]\n",
        "top15_volume = top15_test[('Volume',)]\n",
        "new_columns = top15_volume.columns.map(lambda x: x.split('-')[0] + '-volume')\n",
        "top15_volume = top15_volume.rename(columns=dict(zip(top15_volume.columns, new_columns)))\n",
        "\n",
        "new_columns = top15_volume.columns.map(lambda x: x.split('-')[0] + '-volume')\n",
        "top15_volume = top15_volume.rename(columns=dict(zip(top15_volume.columns, new_columns)))\n",
        "\n",
        "\n",
        "col = list(top15_close.columns)+ list(top15_volume.columns)\n",
        "top15_crypto = pd.DataFrame(np.hstack((top15_close , top15_volume)))\n",
        "top15_crypto.columns =col\n",
        "\n",
        "top15_crypto.index = top15_test.index\n",
        "top15_crypto = top15_crypto.rename(columns={'BTC-USD': 'price_usd_close'})\n",
        "top15_crypto = top15_crypto.fillna(method='ffill')\n",
        "\n",
        "xtest = top15_crypto.price_usd_close[-(x_len+1):]\n",
        "condtest = top15_crypto.drop(\"price_usd_close\",axis=1)[-(x_len+1):]\n",
        "ytest = top15_crypto.price_usd_close[-(x_len+1):]\n",
        "\n",
        "MMScaler_test = MinMaxScaler()\n",
        "SScaler_test = StandardScaler()\n",
        "condscaler_test = StandardScaler()\n",
        "\n",
        "X_trans_test = SScaler_test.fit_transform(xtest.values.reshape(-1, 1))\n",
        "c_trans_test = condscaler_test.fit_transform(condtest.values)\n",
        "y_trans_test = MMScaler_test.fit_transform(ytest.values.reshape(-1, 1))\n",
        "\n",
        "X_trans_test = reshape_df(X_trans_test,90)\n",
        "c_trans_test = reshape_df(c_trans_test,90)\n",
        "y_trans_test = reshape_df(y_trans_test,90)\n",
        "\n",
        "X_trans_test = Variable(torch.Tensor(X_trans_test).to(device))\n",
        "c_trans_test = Variable(torch.Tensor(c_trans_test).to(device))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nb7my5gvfBVC"
      },
      "outputs": [],
      "source": [
        "# checkpoint = torch.load(f'./weights/model_epoch_{best_epoch}.pth')\n",
        "# myModel.load_state_dict(checkpoint['state_dict'])\n",
        "checkpoint = torch.load('./weights/model_epoch_53.pth', map_location=torch.device('cpu'))\n",
        "myModel.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_predict = myModel(X_trans_test , c_trans_test)  # get the last sample\n",
        "test_predict = MMScaler_test.inverse_transform(test_predict.cpu().detach().numpy())\n",
        "# first_day_test_predict = test_predict[:, 0]  # get the first day of the 7-day predictions\n",
        "\n",
        "print(test_predict[0][0],test_predict[1][0])#,test_predict[2][0],test_predict[3][0],test_predict[4][0])\n",
        "# print(\"\",test_predict[0],\"\\n 00000.000\",test_predict[1])\n",
        "\n",
        "from datetime import timedelta\n",
        "for i in range(7):\n",
        "    t = pd.Timestamp(top15_test.index[-1])\n",
        "    t = t + timedelta(hours=4+i, minutes=30)\n",
        "    print(\"at \",t.strftime('%Y-%m-%d %H:%M:%S'),\" predicted price: \",test_predict[-1][i])\n",
        "\n",
        "#2023-07-02 20:00:00 30559.683594\n",
        "\n",
        "print(test_predict[1][0]- test_predict[0][0] > 0)\n",
        "for i in range(6):\n",
        "    print(test_predict[-1][i+1]- test_predict[-1][i] > 0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmqNB3GTUzpV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMLiQ9AeV5IY"
      },
      "outputs": [],
      "source": [
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir logs\n",
        "\n",
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir /content/drive/MyDrive/logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gvBidQltMHB"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-CaD7CKRNHr"
      },
      "outputs": [],
      "source": [
        "# !pip install pytorch-model-summary\n",
        "# import pytorch_model_summary as pms\n",
        "# pms.summary(myModel, torch.zeros(X.shape[0], 30, X.shape[1]).to(device),torch.zeros(X.shape[0], 30, X.shape[1]).to(device), show_input=True, print_summary=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOM8dKj_9_cq"
      },
      "outputs": [],
      "source": [
        "checkpoint = torch.load(f'./weights/model_epoch_{best_epoch}.pth')\n",
        "myModel.load_state_dict(checkpoint['state_dict'])\n",
        "# DataX_test, DataC_test, Datay_test\n",
        "\n",
        "predict = myModel(DataX_test, DataC_test)\n",
        "predict = predict.detach().cpu().numpy()\n",
        "predict = MMScaler_test.inverse_transform(predict)\n",
        "# predict = predict[-100:]\n",
        "true = np.squeeze(Datay_test)\n",
        "true = MMScaler_test.inverse_transform(true.cpu())\n",
        "# true = true[-100:]\n",
        "\n",
        "# calculate the daily returns based on the predicted and true prices\n",
        "predict_returns = (predict[:, 1:] - predict[:, :-1]) / predict[:, :-1]\n",
        "true_returns = (true[:, 1:] - true[:, :-1]) / true[:, :-1]\n",
        "\n",
        "# calculate the position you would have taken based on your prediction\n",
        "threshold = 0.00001  # 1% return threshold\n",
        "predict_position = np.where(predict_returns > threshold, 1, -1)\n",
        "true_position = np.where(true_returns > threshold, 1, -1)\n",
        "\n",
        "# define a function to simulate the portfolio based on the predicted or true returns\n",
        "def simulate_portfolio(position, returns, initial_capital):\n",
        "    # calculate the daily profit and loss based on the position you took and the daily returns\n",
        "    transaction_cost = 0.001  # 0.1% per transaction\n",
        "    position_percentage = 0.2  # 10% of capital\n",
        "    pnl = position * (initial_capital * position_percentage) * (returns - transaction_cost)\n",
        "\n",
        "    # calculate the cumulative PnL over the entire period\n",
        "    cumulative_pnl = np.cumsum(pnl)\n",
        "\n",
        "    # calculate the final portfolio value\n",
        "    final_value = initial_capital + cumulative_pnl[-1]\n",
        "\n",
        "    # calculate the profit percentage\n",
        "    profit_percentage = ((final_value - initial_capital) / initial_capital) * 100\n",
        "\n",
        "    return final_value, profit_percentage\n",
        "\n",
        "# simulate the portfolio based on the predicted and true returns for different initial capital values\n",
        "for cap in [1000, 2000, 5000, 10000]:\n",
        "    print(f\"\\nFor initial capital of {cap}:\")\n",
        "    predict_final_value, predict_profit_percentage = simulate_portfolio(predict_position, predict_returns, cap)\n",
        "    true_final_value, true_profit_percentage = simulate_portfolio(true_position, true_returns, cap)\n",
        "    print(f\"Predicted final value   : {predict_final_value:.2f} with {predict_profit_percentage:.2f}% profit\")\n",
        "    print(f\"True final value        : {true_final_value:.2f} with {true_profit_percentage:.2f}% profit\")\n",
        "    print(f\"(Predicted/True)% profit: {predict_final_value*100/true_final_value:.2f}% profit\")\n",
        "    print(\"-----------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rw96MOw3EfCq"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'kernel_size_EN': [2,3,4],\n",
        "    'kernel_size_DE': [2,3,4],\n",
        "    'hidden_size_lstm': [15],\n",
        "    'num_levels_en': [4,5,6],\n",
        "    'num_levels_de': [4,5,6],\n",
        "    'num_layers_lstm': [6],\n",
        "    'num_layers_gru': [4]\n",
        "}\n",
        "loss_fn = NRMSELoss\n",
        "lr_decay_epoch = 41\n",
        "learning_rate = 0.01\n",
        "weight_decay = 0.0001\n",
        "def NRMSELoss_test(yhat,y): #NRMSE\n",
        "    return (np.sqrt(np.mean(np.square(yhat-y))))/ (np.max(y) - np.min(y))\n",
        "\n",
        "def run_experiment(params):\n",
        "    myModel = TSASeriesNet.ANNmodel(X_train_tensors_final.shape[1], C_train_tensors_final.shape[2], X_train_tensors_final.shape[2],y_train_tensors.shape[1],\n",
        "                                    params['num_levels_en'], params['num_levels_de'], params['kernel_size_EN'],\n",
        "                                    params['kernel_size_DE'], 2, params['hidden_size_lstm'],\n",
        "                                    params['num_layers_lstm'], params['num_layers_gru']).to(device)\n",
        "\n",
        "    optimiser = torch.optim.Adam(myModel.parameters(),weight_decay=0.0001)#, weight_decay=weight_decay, lr=learning_rate)\n",
        "\n",
        "    loss_train_show ,loss_valid_show , best_valid_loss , best_epoch= training_loop(n_epochs=n_epochs,\n",
        "                                  learning_rate = learning_rate,\n",
        "                                  lr_decay_epoch = lr_decay_epoch,\n",
        "                                  network=myModel,\n",
        "                                  optimiser=optimiser,\n",
        "                                  loss_fn=loss_fn,\n",
        "                                  X_train=X_train_tensors_final,\n",
        "                                  Condition_train = C_train_tensors_final,\n",
        "                                  y_train=y_train_tensors,\n",
        "                                  X_test=X_test_tensors_final,\n",
        "                                  Condition_test  = C_test_tensors_final,\n",
        "                                  y_test=y_test_tensors,\n",
        "                                  batch_size = batch_size)\n",
        "\n",
        "    checkpoint = torch.load(f'./weights/model_epoch_{best_epoch}.pth')\n",
        "    myModel.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_predict = myModel(DataX_test, DataC_test)  # get the last sample\n",
        "    test_predict = test_predict.detach().cpu().numpy()\n",
        "\n",
        "    test_target = Datay_test.detach().cpu().numpy()  # last sample again\n",
        "\n",
        "\n",
        "\n",
        "    # NRMSELoss_res = NRMSELoss_test(test_predict[:,0] ,first_day_test_target.reshape(-1) )\n",
        "    NRMSELoss_res = NRMSELoss_test(test_predict ,test_target.reshape(test_target.shape[0],test_target.shape[1]) )\n",
        "    print(\"****TEST*** \",NRMSELoss_res)\n",
        "\n",
        "\n",
        "\n",
        "    return loss_valid_show[-1]\n",
        "\n",
        "import itertools\n",
        "\n",
        "def grid_search(param_grid):\n",
        "    keys, values = zip(*param_grid.items())\n",
        "    min_loss = float('inf')\n",
        "    best_params = None\n",
        "\n",
        "    for v in itertools.product(*values):\n",
        "        params = dict(zip(keys, v))\n",
        "        print(f\"Running experiment with parameters: {params}\")\n",
        "        loss = run_experiment(params)\n",
        "        print(f\"Validation loss: {loss}\")\n",
        "\n",
        "        if loss < min_loss:\n",
        "            min_loss = loss\n",
        "            best_params = params\n",
        "\n",
        "    return best_params, min_loss\n",
        "\n",
        "best_params, min_loss = grid_search(param_grid)\n",
        "print(f\"Best parameters: {best_params}\")\n",
        "print(f\"Minimum validation loss: {min_loss}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhglBRzIVzSg"
      },
      "outputs": [],
      "source": [
        "import backtrader as bt\n",
        "import pandas as pd\n",
        "\n",
        "checkpoint = torch.load(f'./weights/model_epoch_{best_epoch}.pth')\n",
        "myModel.load_state_dict(checkpoint['state_dict'])\n",
        "# DataX_test, DataC_test, Datay_test\n",
        "\n",
        "predict = myModel(DataX_test, DataC_test)\n",
        "predict = predict.detach().cpu().numpy()\n",
        "predict = pd.DataFrame(MMScaler_test.inverse_transform(predict))\n",
        "predict.index = data.index[-106:]\n",
        "predict = predict.rename_axis(\"datetime\")\n",
        "\n",
        "# predict = predict[-100:]\n",
        "true = np.squeeze(Datay_test)\n",
        "true = pd.DataFrame(MMScaler_test.inverse_transform(true.cpu()))\n",
        "true.index = data.index[-106:]\n",
        "true = true.rename_axis(\"datetime\")\n",
        "\n",
        "# Load the predicted data\n",
        "predicted_data = pd.DataFrame(predict[0])\n",
        "predicted_data = predicted_data.rename(columns={0: 'predicted'})\n",
        "# predicted_data.index = data.index[-143:]\n",
        "# Load the true data\n",
        "true_data = pd.DataFrame(true[0])\n",
        "true_data = true_data.rename(columns={0: 'true'})\n",
        "# true_data.index = data.index[-143:]\n",
        "# Define the strategy\n",
        "class NeuralNetStrategy(bt.Strategy):\n",
        "    params = (\n",
        "        ('threshold', 0.05),\n",
        "    )\n",
        "\n",
        "    def __init__(self):\n",
        "        self.data_predicted = self.datas[0]\n",
        "        self.data_true = self.datas[1]\n",
        "\n",
        "    def next(self):\n",
        "        if self.data_predicted.close[0] > self.data_true.close[0] * (1 + self.params.threshold):\n",
        "            self.buy()\n",
        "        elif self.data_predicted.close[0] < self.data_true.close[0] * (1 - self.params.threshold):\n",
        "            self.sell()\n",
        "\n",
        "# Create a cerebro instance\n",
        "cerebro = bt.Cerebro()\n",
        "\n",
        "# Add the predicted data\n",
        "predicted_data_feed = bt.feeds.PandasData(dataname=predicted_data)\n",
        "cerebro.adddata(predicted_data_feed, name='predicted')\n",
        "\n",
        "# Add the true data\n",
        "true_data_feed = bt.feeds.PandasData(dataname=true_data)\n",
        "cerebro.adddata(true_data_feed, name='true')\n",
        "\n",
        "# Add the strategy\n",
        "cerebro.addstrategy(NeuralNetStrategy, threshold=0.05)\n",
        "\n",
        "# Set the cash and commission\n",
        "cerebro.broker.setcash(1000.0)\n",
        "cerebro.broker.setcommission(commission=0.001)\n",
        "\n",
        "# Run the backtest\n",
        "cerebro.run()\n",
        "\n",
        "# Print the final portfolio value\n",
        "print('Final Portfolio Value: %.2f' % cerebro.broker.getvalue())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}